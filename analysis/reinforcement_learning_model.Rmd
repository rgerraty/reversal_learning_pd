---
title: "Parkinsonâ€™s RL"
author: "Raphael Gerraty"
output: html_document
---


```{r, eval=FALSE}
library("lme4")
library("doBy")
library("ggplot2")
library("rstan")
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

#read in learning data from MATLAB output
learndata=read.table("~/Dropbox/reversal_learning_pd/Subjects/LearnGroup_newest.txt", header=FALSE, sep="")

#add header row 
headerlearn=c("SubNum", "Trial","Day",'Med','StimOnLeft','rewCat','Resp','RewProb','Rew','Optimal','ChosenCat','Old','RT', 'Bin')

names(learndata)<-headerlearn

head(learndata)
#will need to remove trials where data was saved incorrectly
#Subject 822 Trials 22-30

# effect coding for reward and day and med
learndata$RewEff<-(2*learndata$Rew-1)/2 #RewEff is -.5/.5 
learndata$DayEff<-(2*learndata$Day-3)/2 #went from 1/2 to -.5/.5 
learndata$MedEff<-(2*learndata$Med-1)/2 #medEff is -.5/.5 

#abs(choice-3) is a hack to get unchosen from chosen [1,2];
learndata$UnChosenCat<-abs(learndata$ChosenCat-3)

#Change DV coding to p(Choose Cat2)
learndata$ChooseTwo<-(learndata$ChosenCat==2)+0

#code lucky category .5/-.5
learndata$LuckCatTwo<-((learndata$rewCat==2)-(learndata$rewCat==1))/2
learndata$LuckCatTwo[is.na(learndata$LuckCatTwo)]<-0
```

Set up data for Stan Model

```{r, eval=FALSE}
#set up variables in subjects by trials format for Stan
subs = unique(learndata$SubNum)
NS = length(subs);
MT=max(learndata$Trial*learndata$Day);
NT = array(0,NS);
choice = array(0,c(NS,MT));
unchoice=choice;
cat_choice=choice;
rew = array(0.0,c(NS,MT));
day = rew;
med = rew;

#convert data to subjects by trials format for Stan
for (i in 1:NS) {
  NT[i] = nrow(subset(learndata,SubNum==subs[i]));
  
  #choice and reward history
  choice[i,1:NT[i]] = subset(learndata,SubNum==subs[i])$ChosenCat;
  unchoice[i,1:NT[i]] = subset(learndata,SubNum==subs[i])$UnChosenCat;
  rew[i,1:NT[i]] = subset(learndata,SubNum==subs[i])$Rew;
  
  #based on choosing category 2
  cat_choice[i,1:NT[i]] = subset(learndata,SubNum==subs[i])$ChooseTwo;
  
  #Day and Med
  day[i,1:NT[i]] = subset(learndata,SubNum==subs[i])$DayEff;
  med[i,1:NT[i]] = subset(learndata,SubNum==subs[i])$MedEff;
}

#for skipping missed trials
choice[is.na(choice)]<--1
choice[choice==0]<--1
unchoice[is.na(unchoice)]<--1
unchoice[choice==0]<--1
rew[is.na(rew)]<--1
cat_choice[is.na(cat_choice)]<--1
day[day==0] <- -1
med[med==0] <- -1

#standard rl model fit heirarchically in Stan
standata = list(NS=NS, NC=2,K=4, MT=MT, NT= NT, choice=choice, cat_choice=cat_choice, rew=rew, day=day,med=med )
stan_fit <- stan(file = '~/GitHub/reversal_learning_pd/analysis/rl.stan', data = standata, iter = 1250, warmup = 250, chains = 4,adapt_delta=.9)
save(stan_fit,file='~/GitHub/reversal_learning_pd/analysis/stanfit_rl_PD')

#maybe make model with different alphas for ON vs OFF?


#lme4 models for comparison

```


```{r, eval=FALSE}
fit2<-load('~/GitHub/reversal_learning_pd/analysis/stanfit_rl_PD')
fit_extract<-extract(standard_fit,permute=T)
Qvals<-apply(fit_extract$Q,c(2,3,4),mean)


#Separate Q value arrays for chosen and unchosen options
Q_chosen<-matrix(0,dim(Qvals)[2],dim(Qvals)[1])
Q_unchosen<-Q_chosen

for(i in 1:dim(Qvals)[1]){
  for(j in 1:dim(Qvals)[2]){
    if(choice[i,j]>0){
      Q_chosen[j,i]<-Qvals[i,j,choice[i,j]]
      Q_unchosen[j,i]<-Qvals[i,j,unchoice[i,j]]
    } 
      else{
      Q_chosen[j,i]<-NA
      Q_unchosen[j,i]<-NA
    }
  }

}
pe<-t(apply(fit_extract$delta,c(2,3),mean))

alpha<-apply(fit_extract$alpha,2,mean)
beta<-apply(fit_extract$beta,c(2,3),mean)
Sigma<-apply(fit_extract$Sigma,c(2,3),mean)
Omega<-apply(fit_extract$Omega,c(2,3),mean)


#Summary for group level effects and covariance
summary(fit_extract$b_mean)


#pairs(hybrid1_fit,pars="b_mean",labels=c("Intercept","Inverse Temp","Familiarity Bias","Episodic Value"))


hist(fit_extract$b_mean[,2],xlab="Average Incremental Effect",main=NULL)
hist(fit_extract$b_mean[,4],xlab="Average Drug Effect",main=NULL)
hist(fit_extract$b_mean[,3],xlab="Average Day Effect",main=NULL)
hist(fit_extract$alpha,xlab="Average Alpha",main=NULL)

#plot mean estimates of subject-level effects
hist(beta[,2],xlab="Incremental Effect",main=NULL)


#plot posterior uncertainty for subject-level estimates
plot(hybrid1_fit,pars=c("beta[1,2]","beta[2,2]",
                        "beta[3,2]","beta[4,2]",
                        "beta[5,2]","beta[6,2]",
                        "beta[7,2]","beta[8,2]",
                        "beta[9,2]","beta[10,2]",
                        "beta[11,2]","beta[12,2]",
                        "beta[13,2]","beta[14,2]",
                        "beta[15,2]","beta[16,2]",
                        "beta[17,2]","beta[18,2]",
                        "beta[19,2]","beta[20,2]",
                        "beta[21,2]","beta[22,2]",
                        "beta[23,2]","beta[24,2]",
                        "beta[25,2]","beta[26,2]",
                        "beta[27,2]","beta[28,2]",
                        "beta[29,2]","beta[30,2]","beta[31,2]"))

plot(hybrid1_fit,pars=c("beta[1,3]","beta[2,3]",
                        "beta[3,3]","beta[4,3]",
                        "beta[5,3]","beta[6,3]",
                        "beta[7,3]","beta[8,3]",
                        "beta[9,3]","beta[10,3]",
                        "beta[11,3]","beta[12,3]",
                        "beta[13,3]","beta[14,3]",
                        "beta[15,3]","beta[16,3]",
                        "beta[17,3]","beta[18,3]",
                        "beta[19,3]","beta[20,3]",
                        "beta[21,3]","beta[22,3]",
                        "beta[23,3]","beta[24,3]",
                        "beta[25,3]","beta[26,3]",
                        "beta[27,3]","beta[28,3]",
                        "beta[29,3]","beta[30,3]","beta[31,3]"))

plot(hybrid1_fit,pars=c("beta[1,4]","beta[2,4]",
                        "beta[3,4]","beta[4,4]",
                        "beta[5,4]","beta[6,4]",
                        "beta[7,4]","beta[8,4]",
                        "beta[9,4]","beta[10,4]",
                        "beta[11,4]","beta[12,4]",
                        "beta[13,4]","beta[14,4]",
                        "beta[15,4]","beta[16,4]",
                        "beta[17,4]","beta[18,4]",
                        "beta[19,4]","beta[20,4]",
                        "beta[21,4]","beta[22,4]",
                        "beta[23,4]","beta[24,4]",
                        "beta[25,4]","beta[26,4]",
                        "beta[27,4]","beta[28,4]",
                        "beta[29,4]","beta[30,4]","beta[31,4]"))


#glmer likelihood approximation for comparison
#also realized need to look at interaction, so bayes model needs
#to look like this
me_PD<-glmer(ChooseTwo ~ LuckCatTwo*MedEff*DayEff
             + (LuckCatTwo*MedEff | SubNum),data=learndata,family=binomial)



```

